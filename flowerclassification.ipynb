{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178c06a9-b60f-467a-a9f9-b24da4d95a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "======================================\n",
      "MODEL: Logistic Regression\n",
      "Accuracy: 93.33%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.90      0.90      0.90        10\n",
      "   virginica       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  1  9]]\n",
      "\n",
      "======================================\n",
      "MODEL: K-Nearest Neighbors\n",
      "Accuracy: 93.33%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.83      1.00      0.91        10\n",
      "   virginica       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0 10  0]\n",
      " [ 0  2  8]]\n",
      "\n",
      "======================================\n",
      "MODEL: Support Vector Machine (SVM)\n",
      "Accuracy: 96.67%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.90      0.95        10\n",
      "   virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 10]]\n",
      "\n",
      "======================================\n",
      "MODEL: Decision Tree\n",
      "Accuracy: 93.33%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.90      0.90      0.90        10\n",
      "   virginica       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  1  9]]\n",
      "\n",
      "======================================\n",
      "MODEL: Random Forest\n",
      "Accuracy: 90.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       0.82      0.90      0.86        10\n",
      "   virginica       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  2  8]]\n",
      "\n",
      "======================================\n",
      "MODEL: Naive Bayes\n",
      "Accuracy: 96.67%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.90      0.95        10\n",
      "   virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 10]]\n",
      "\n",
      "\n",
      "======== MODEL COMPARISON (Accuracy) ========\n",
      "Logistic Regression            : 93.33%\n",
      "K-Nearest Neighbors            : 93.33%\n",
      "Support Vector Machine (SVM)   : 96.67%\n",
      "Decision Tree                  : 93.33%\n",
      "Random Forest                  : 90.00%\n",
      "Naive Bayes                    : 96.67%\n",
      "\n",
      "Sample Input: [5.1 3.5 1.4 0.2]\n",
      "Predicted Class: setosa\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# IRIS FLOWER CLASSIFICATION USING MULTIPLE ALGORITHMS\n",
    "# ==========================================================\n",
    "\n",
    "# 1. Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 2. Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data          # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target        # Labels: 0 = setosa, 1 = versicolor, 2 = virginica\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", target_names)\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 4. Feature scaling (important for distance-based and SVM models)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 5. Define multiple classification algorithms\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Support Vector Machine (SVM)\": SVC(kernel='rbf', probability=True),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# 6. Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # For tree-based models, scaling is not mandatory, but we can still use scaled data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"MODEL:\", name)\n",
    "    print(\"Accuracy: {:.2f}%\".format(acc * 100))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 7. Show a comparison of all models\n",
    "print(\"\\n\\n======== MODEL COMPARISON (Accuracy) ========\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"{name:30s} : {acc*100:.2f}%\")\n",
    "\n",
    "# 8. Example: Using the best model for a single prediction\n",
    "# Here, we simply choose Random Forest as an example\n",
    "\n",
    "best_model = models[\"Random Forest\"]\n",
    "\n",
    "sample = np.array([[5.1, 3.5, 1.4, 0.2]])   # Example input (looks like setosa)\n",
    "sample_scaled = scaler.transform(sample)\n",
    "sample_pred = best_model.predict(sample_scaled)\n",
    "\n",
    "print(\"\\nSample Input:\", sample[0])\n",
    "print(\"Predicted Class:\", target_names[sample_pred[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ded2df-1c34-4472-95e1-2ffea926020a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
